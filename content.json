{"meta":{"title":"CyouGuang'Blog","subtitle":"陈柚光的小狗窝","description":null,"author":"CyouGuang","url":"https://blog.chenyouguang.cn","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-06-28T16:29:25.849Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"404.html","permalink":"https://blog.chenyouguang.cn/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-06-28T16:29:25.849Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"about/index.html","permalink":"https://blog.chenyouguang.cn/about/index.html","excerpt":"","text":"普通大学狗一只GitHub:https://github.com/sbtobb 博客引擎:Hexo主题:Volantis评论系统:utterancServer:Github PageCDN:又拍云联盟"},{"title":"所有分类","date":"2020-06-28T16:29:25.849Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"categories/index.html","permalink":"https://blog.chenyouguang.cn/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2020-06-28T16:29:25.849Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"friends/index.html","permalink":"https://blog.chenyouguang.cn/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 友情链接: 123avatar: https://cdn.jsdelivr.net/gh/sbtobb/CDN/blog/20200310235037.pngname: CyouGuang's Blogurl: https://blog.chenyouguang.com 需要互换友联的朋友可以在下方评论留言喔"},{"title":"所有标签","date":"2020-06-28T16:29:25.849Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"tags/index.html","permalink":"https://blog.chenyouguang.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"考研咨询政策数据挖掘","slug":"考研资讯政策数据挖掘","date":"2019-05-10T04:00:23.000Z","updated":"2020-06-28T11:23:19.000Z","comments":true,"path":"2019/05/10/考研资讯政策数据挖掘/","link":"","permalink":"https://blog.chenyouguang.cn/2019/05/10/%E8%80%83%E7%A0%94%E8%B5%84%E8%AE%AF%E6%94%BF%E7%AD%96%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","excerpt":"​ 近年来，考研人数不断添加，考研对于我们来说是一个热点不断的话题，对于考研资讯信息不够集中，数据量过大，考生无法准确的判断某个时期热点的资料，因此越来越多考生想能够快速得了解考研资讯、国家政策以及院校政策。 ​ 本项目通过对招生资讯、国家政策、院校政策进行总体数据挖掘以及分别进行数据挖掘，对院校的资讯热点进行提取绘制成词云图。 ​ 考生通过数据绘制出词云图，可以清楚的了解到近年来院校关注的重点是什么，以及近年来考研相关的国家政策热点，借助这些信息，能够帮助到我们广大考生进行更好选择所需要关注的热点信息，更为方便的了解到国家政策。","text":"​ 近年来，考研人数不断添加，考研对于我们来说是一个热点不断的话题，对于考研资讯信息不够集中，数据量过大，考生无法准确的判断某个时期热点的资料，因此越来越多考生想能够快速得了解考研资讯、国家政策以及院校政策。 ​ 本项目通过对招生资讯、国家政策、院校政策进行总体数据挖掘以及分别进行数据挖掘，对院校的资讯热点进行提取绘制成词云图。 ​ 考生通过数据绘制出词云图，可以清楚的了解到近年来院校关注的重点是什么，以及近年来考研相关的国家政策热点，借助这些信息，能够帮助到我们广大考生进行更好选择所需要关注的热点信息，更为方便的了解到国家政策。 1 研究背景及目的1.1 背景​ 近年来，考研人数不断添加，考研对于我们来说是一个热点不断的话题，对于考研资讯信息不够集中，数据量过大，考生无法准确的判断某个时期热点的资料，因此越来越多考生想能够快速得了解考研资讯、国家政策以及院校政策。 1.2 考研资讯政策数据资源(1)数据来源 中国研究生招生信息网 (2)数据规模 爬取了近 5 年的招生资讯、国家政策、院校政策，共 5600 篇文章，约 750 万字。 (3)数据特征分析 文本型数据，招生资讯占比 62%，国家政策占比 4%，院校政策占比 34%。 1.3 考研资讯政策数据挖掘目的和意义(1)数据挖掘的目标 ​ 通过对招生资讯、国家政策、院校政策进行总体数据挖掘以及分别进行数据挖掘，对 院校的资讯热点进行提取绘制成词云图。 (2)实际应用价值分析 ​ 考生通过数据绘制出词云图，可以清楚的了解到近年来院校关注的重点是什么，以及近年来考研相关的国家政策热点，借助这些信息，能够帮助到我们广大考生进行更好选择所需要关注的热点信息，更为方便的了解到国家政策。 2 考研资讯政策数据挖掘系统设计2.1 系统总体设计2.1.1 系统设计目标​ 通过爬虫爬取信息，将信息按照规定格式进行存储，将数据读入，构建语料库，进行中文分词，再进行词频统计分析，绘制词云图和柱状图，进行保存。 2.1.2 总体流程图 ​ 系统由爬虫子系统和数据挖掘子系统组成，流程 2.2 系统功能模块设计2.2.1 数据抓取模块设计​ 给予开始位置、模式，即可开始抓取到研招网的招生资讯、院校政策、国家政策 的列表。 12345模块:requests, bs4函数:http 请求函数:requests.get(url) url:待获取网页源代码的 URLhtml 解析函数:bs4.BeautifulSoup(content,model) content:待解析的 html 文本，model:解释器 2.2.2 数据解析模块设计​ 给予 url，即可开始解析网页中的文章数据。 123模块:requests, bs4函数:数据标签查找函数:BeautifulSoup.find(tag,id) tag:html 标签 id:过滤器 2.2.3 数据保存模块设计​ 给予待保存数据、文件路径，将会将数据文件按格式保存在磁盘中 1234模块:os函数:打开文件函数 open(path, model) path:打开的文档路径 model:打开模式(wt 为写 入模式)写入数据函数:f.write(article) article:待写入的数据 2.2.4 数据导入模块设计​ 遍历文件夹，将文本数据从文件夹中取出来，并分析出出文本时间、文本类型， 返回拥有多个返回值 filePaths,fileContents,dateTimes,Species 都为 list 类型。 1234模块:os，codecs函数:文件遍历函数:os.walk(path) path:待遍历文件路径读取文件函数 codecs.open(filePath,&#39;r&#39;,&#39;utf-8&#39;) 文件路径 打开模式 文件编码 2.2.5 构建语料库模块设计​ 给予数据文件夹路径，调用数据导入模块，取得文件数据，构建语料库。 123模块: pandas函数:创建数据框函数:pandas.DataFrame(&#123;&#125;) 参数:字典类型 2.2.6 中文分词模块设计 TextRank 算法分析: 类似于 PageRank 的思想，将文本中的语法单元视作图中的 节点，如果两个语法单元存在一定语法关系，则这两个语法单元在图中就会有一条边 相互连接，通过一定的迭代次数，最终不同的节点会有不同的权重，权重高的语法单 元可以作为关键词。节点的权重不仅依赖于它的入度结点，还依赖于这些入度结点的 权重，入度结点越多，入度结点的权重越大，说明这个结点的权重越高。 ​ 给予语料库，对语料库文本进行中文分词，由于数据量较大，采用多线程方式执 行。 123456789模块: concurrent, jieba numpy函数:线程池创建函数:ThreadPoolExecutor(max_workers) max_workers: 同时进行的线程个数等待线程完成函数:wait(all_task, return_when&#x3D;ALL_COMPLETED) all_task:线程池句柄 return_when:等待类型 ALL_COMPLETED 全部线程执行完毕读取 csv 数据函数:pandas.read_csv(path, encoding, index_col) path:路径, encoding:编码, index_col 序列 Textran:jieba.analyse.textrank(content,topK&#x3D;50,withWeight&#x3D;False,allowPOS&#x3D;(&#39;ns&#39;, &#39;n&#39;,&#39;vn&#39;, &#39;v&#39;)) content:待分割的文本 2.2.7 词频统计模块设计​ 给予中文分词数据框、过滤词频，对分词的数据进行合并统计。 1234模块: pandas函数:词频合并函数:segment_dataframe.groupby(&#39;word&#39;)[&#39;count&#39;].sum() 参数(’word’):抽 取 word 字段 [&#39;count&#39;].sum() 计算 count 数据的和排序函数:ser_word.sort_values(ascending&#x3D;False) ascending 是否为升序 元组转换函数:zip(a,b) 打包 a,b 为元组数据 3 系统实现3.1 数据抓取 向服务器发送请求-&gt;编码猜测-&gt;对 html 文档进行遍历-&gt;组合成 list 数据 12345678910111213141516171819202122232425262728293031323334# 定义获取资讯列表函数def get_consultant_list(start, model): # 返回结果 result = [] # 发送http请求 response = requests.get(base_url + \"/kyzx/\"+model+\"/?start=\" + str(start)) # 判断是否请求成功 if response.status_code != 200: print(\"get_consultant_list请求失败\") return result # 设置相应数据的编码为猜测的编码 response.encoding = response.apparent_encoding # 使用BeautifulSoup进行煲汤 解析器为 html.parser soup = bs4.BeautifulSoup(response.text, \"html.parser\") # 寻找标签 ul css:news-list 的子标签 liList = soup.find(\"ul\", class_=\"news-list\").children # 对li标签进行遍历 for li in liList: # 判断类型是否为Tag if isinstance(li, bs4.element.Tag): e = [] # 提取出文章日期信息 date = li.span.string # 提取出文章标题 title = li.a.string # 提取出文章网址 url = li.a['href'] if date is None or title is None or url is None: continue e.append(title) e.append(url) e.append(date) result.append(e) return result ​ 运行结果无误 ​ 采用 requests 库构建 HTTP 请求，再使用 BeautifulSoup 库进行 html 解析，找到存放文章 url 的标签之后，对该标签进行遍历，并过滤掉无用的标签，最后再不断拆分 html 数据，组合成自己 的数据结构。 3.2 数据解析 向服务器发送请求-&gt;编码猜测-&gt;对 html 文档进行解析-&gt;遍历 p 标签-&gt;组合 p 标签数据 123456789101112131415161718192021# 定义获取文章函数def get_article(url): # 发送http请求 response = requests.get(base_url + url) # 判断是否请求成功 if response.status_code != 200: print(\"response 请求失败\") return \"\" # 设置相应数据的编码为猜测的编码 response.encoding = response.apparent_encoding # 使用BeautifulSoup进行煲汤 解析器为 html.parser soup = bs4.BeautifulSoup(response.text, \"html.parser\") # 寻找标签 div id:article_dnull div_element = soup.find(\"div\", id=\"article_dnull\") result = \"\" # 找到 div 标签下所有的p标签并进行遍历 for p in div_element.findAll(\"p\"): if p.string is None: continue result = result + p.string return result ​ 运行结果无误 ​ 分析:采用 requests 库构建 HTTP 请求，使用 BeautifulSoup 库进行 html 解析，定位到存放文章的 标签 div 上面，再对其下的 p 标签进行遍历，取出每一句的文章数据进行组合。 3.3 数据保存 替换掉特殊字符-&gt;取出文章数据-&gt;判断文章是否为空-&gt;写入磁盘-&gt;打印 log 1234567891011121314151617# 定义保存文章函数def save_article(l,path): # 替换掉标题中的特殊字符 避免不能保存 title = l[0].replace(\"：\", \"\") title = title.replace(\":\", \"\") title = title.replace(\"/\", \"\") totalPath = path + l[2] + \"-\" + title + \".txt\" # 取出文章数据 article = get_article(l[1]) # 判断文章是否为空 if article == \"\": return # 保存数据 with open(totalPath, 'wt') as f: f.write(article) # 打印log print(l[2] + \"-\" + l[0]) ​ 分析:取出文章的标题信息，将标题信息的特殊字符过滤掉避免无法保存到计算机里，判断文章 数据是否为空值，写出文本文件。 3.4 数据导入 遍历文件夹-&gt;截取时间元组-&gt;读入文本数据-&gt;判断文本类型-&gt;组合数据返回 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import os;import os.path;import codecs;def file_traversal(path): \"\"\"遍历文件夹 遍历数据文件夹 Args: path: 待遍历的文件路径 Returns: 拥有多个返回值 filePaths,fileContents,dateTimes,Species 都为list类型 \"\"\" # 文件路径 filePaths = [] # 文件内容 fileContents = [] # 日期 years = [] months = [] days = [] # 类别 Species=[] for root, dirs, files in os.walk(path): for name in files: if name == \".DS_Store\": continue #从文件名中截取出时间元组 time_str = name[:10] year = int(time_str[:4]) month = int(time_str[5:7]) day = int(time_str[9:11]) filePath = os.path.join(root,name) filePaths.append(filePath) f = codecs.open(filePath,'r','utf-8') fileContent = f.read() f.close() years.append(year) months.append(month) days.append(day) fileContents.append(fileContent) if root.find('consultant') != -1: Species.append('consultant') elif root.find('countries_policy') != -1: Species.append('countries_policy') elif root.find('school_policy') != -1: Species.append('school_policy') else : Species.append('null') return filePaths,fileContents,Species,years,months,days ​ 运行结果无误 ​ 分析:采用 os.walk 进行文件遍历，使用 codecs.open 读入文本数据，分割出文章文本的发布时 间，通过文件路径名判断文本数据的类型 3.5 数据预处理 将数据读入的返回值构建成语料库 1234567891011121314151617181920212223242526import pandasdef build_corpus(path): \"\"\"构建语料库 构建语料库 Args: path: 待遍历的文件路径 Returns: DateFrame 语料库 \"\"\" filePaths,fileContents,Species,years,months,days = file_traversal(path) # 将filePath、fileContent、dateTime、Species 加载到数据框corpos中，形成语料库 corpos = pandas.DataFrame(&#123; 'filePath':filePaths, 'fileContent':fileContents, 'Species':Species, 'year':years, 'month':months, 'day':days &#125;) return corpos ​ 运行结果无误 ​ 分析:将文件遍历的结构构建成语料库 3.6 文本关键词模型的构建 创建线程池-&gt;遍历语料库-&gt;提交结巴分词任务-&gt;读入停用词-&gt;采用 TextRank 进行分词-&gt;等待线程结束-&gt;组合多线程函数结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import jiebaimport numpyimport jieba.analyse# 定义结巴分词函数def jieba_segment(content): \"\"\"结巴分词 对语料库文本进行中文分词 Args: content: 待切割文本 Returns: List [&#123;'word':a,'count',1&#125;] 分词列表 \"\"\" # 分词词库 segments = [] # 读入停用词 stopwords = pandas.read_csv( \"/Users/Apple/Documents/CodeWork/DataAnalysis/code_week_6/3.1/StopwordsCN.txt\", encoding='utf8', index_col=False ) words = jieba.analyse.textrank(content, topK=50,withWeight=False,allowPOS=('ns', 'n', 'vn', 'v')) for word in words: # 记录全局分词 if word not in stopwords: segments.append(&#123;'word':word, 'count':1&#125;) return segments # 定义中文分词函数from concurrent.futures import ThreadPoolExecutor,wait,ALL_COMPLETEDdef word_segment(corpos): \"\"\"中文分词 对语料库文本进行中文分词，由于数据量较大，采用多线程方式执行 Args: corpos: 语料库 Returns: DateFrame 词频 \"\"\" # 创建线程池 executor = ThreadPoolExecutor(max_workers=20) # 线程池句柄 all_task = [] # 分词词库 segments = [] for index, row in corpos.iterrows(): content = row['fileContent'] all_task.append(executor.submit(jieba_segment,content)) # 等待线程全部完成 wait(all_task, return_when=ALL_COMPLETED) # 将结果汇总 for task in all_task: segments.extend(task.result()) # 构建DateFrame dfSg = pd.DataFrame(segments) return dfSg ​ 运行结果无误 ​ 分析:由于数据量过于庞大，单线程需要等待时间过久，这里采用多线程的方式进行文本分割， 建立线程池后，将待分割的文本数据传入结巴分词函数中，结巴分词采用 TextRank 算法进行文本 分割，等待文本分割完成后，再将数据组合。 3.7 文本关键词模型的训练 词频统计:将分词进行聚合合并-&gt;降序排序-&gt;过滤数据-&gt;转换成元组数据 词云图生成:输入词频元组数据,标题，设置词云图参数，返回词云图对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 定义词频统计函数def word_frequency(segment_dataframe,count_filter = 0): \"\"\"词频统计 对分词的数据进行合并统计 Args: segment_dataframe: 中文分词 count_filter:排除小于过滤的词 Returns: tuple 词频 \"\"\" # 对分词进行合并 ser_word = segment_dataframe.groupby('word')['count'].sum() # 对分词进行排序 nSegStat = ser_word.sort_values(ascending=False) # 过滤 nSegStat = nSegStat[nSegStat &gt;= count_filter] # 转换成元组数据 tup = tuple(zip(nSegStat.index,nSegStat)) return tup # 定义生成词云图函数from pyecharts import options as optsfrom pyecharts.charts import Page, WordCloudfrom pyecharts.globals import SymbolType,CurrentConfig, NotebookTypeCurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_LABdef build_word_cloud(data,title=\"\"): word_cloud = ( WordCloud() # 添加数据和词云图的标题 字大小 .add(series_name=title, data_pair=data, word_size_range=[10, 100]) # 设置图片的参数 .set_global_opts( title_opts=opts.TitleOpts( title=title, title_textstyle_opts=opts.TextStyleOpts(font_size=23) ), tooltip_opts=opts.TooltipOpts(is_show=True), ) ) return word_cloud ​ 运行结果无误 ​ 分析:词频统计将分词数据使用聚合函数进行分类，使用 sort_values 进行降序排序，过滤掉过 小而无法显示的数据，提高词云图渲染的速度。 4 系统测试分析4.1 数据爬取 创建线程池-&gt;设置抓取的资讯-&gt;获取数据列表-&gt;遍历数据列表-&gt;提交爬取文章任务 1234567891011121314151617181920212223242526272829303132333435363738394041# 创建线程池executor = ThreadPoolExecutor(max_workers=20)# 开始位置start = 0go_ahead = True# 招生资讯:kydt 院校政策:yxzc 国家政策:zcdhmodel = \"zcdh\"file_path = \"\"if model == \"kydt\": file_path = \"consultant\"elif model == \"yxzc\": file_path = \"school_policy\"elif model == \"zcdh\": file_path = \"countries_policy\"if file_path == \"\": print(\"请输入合理的爬虫参数\") returnwhile go_ahead: # 获取50个数据列表 clist = get_consultant_list(start,model) # 线程句柄保存 all_task = [] for l in clist: # 读取年份数据 year = (l[2])[:4] if year == \"2012\": go_ahead = False break # 数据分类分年存放 path = os.getcwd()+'/'+file_path+'/'+year+\"/\" # 判断文件夹是否存在 不存在创建一个新的文件夹 if not os.path.exists(path): os.makedirs(path) # 将获取数据函数加入线程池 all_task.append(executor.submit(save_article,l,path)) # 等待所有线程完成 wait(all_task, return_when=ALL_COMPLETED) # 进行下一个开始位置 start = start + 50 ​ 分析:数据量过大且 HTTP 请求需要时间，采用了多线程的方式爬取数据，并将数据按照规定的 格式进行保存，方便数据读入分析。 4.2 总体分析 将数据文件夹传入-&gt;构建语料库-&gt;创建中文分词-&gt;将词频低于 150 次的词过滤，提高词云图的 渲染速度-&gt;创建词云图-&gt;保存图片","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/categories/Python/"},{"name":"研究项目","slug":"研究项目","permalink":"https://blog.chenyouguang.cn/categories/%E7%A0%94%E7%A9%B6%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/tags/Python/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://blog.chenyouguang.cn/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}]},{"title":"使用pyecharts绘制heatmap","slug":"使用pyecharts绘制heatmap","date":"2019-04-02T14:15:34.000Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"2019/04/02/使用pyecharts绘制heatmap/","link":"","permalink":"https://blog.chenyouguang.cn/2019/04/02/%E4%BD%BF%E7%94%A8pyecharts%E7%BB%98%E5%88%B6heatmap/","excerpt":"","text":"实现效果 准备步骤首先需要安装pyecharts库以及地图库在上一篇文章中已经教大家安装了传送门:Anaconda安装pyecharts可视化界面库 大家可以参考一下官方文档:heatmap 数据获取 打算做一个大连民族大学2016级物联网工程专业的生源分布热力图大连民族大学招生网从招生官网中获取到数据(理论上可以使用爬虫直接获取) 省份 人数 广西 10 内蒙古 11 河北 5 贵州 6 吉林 11 黑龙江 2 山东 2 重庆 5 csv数据 123456789省份,人数广西,10内蒙古,11河北,5贵州,6吉林,11黑龙江,2山东,2重庆,5 开始编码1234567891011121314from pyecharts import Geofrom pandas import read_csv# TODO 修改成自己的文件路径df1 = read_csv('people.csv', encoding='UTF-8' )geo = Geo(\"2016级物联网工程生源图\", \"data from iot\", title_color=\"#fff\", title_pos=\"center\", width=1200, height=600, background_color='#404a59')geo.add(\"2016级物联网工程生源图\", df1['省份'], df1['人数'], visual_range=[0, 11], type='heatmap',visual_text_color=\"#fff\", symbol_size=15, is_visualmap=True, is_roam=False)# TODO 修改成自己的生成文件路径geo.render(path=\"./16物联网工程生源.html\") 打开生成文件执行完之后会在py文件所在文件夹下有一个html文件使用现代浏览器打开即可(推荐chrome,火狐) 如果打开html文件只显示南海群岛的话，需要等待一下再进行刷新即可","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/tags/Python/"},{"name":"热力图","slug":"热力图","permalink":"https://blog.chenyouguang.cn/tags/%E7%83%AD%E5%8A%9B%E5%9B%BE/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://blog.chenyouguang.cn/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"Anaconda安装pyecharts可视化界面库","slug":"Anaconda安装pyecharts可视化界面库","date":"2019-04-02T03:34:32.000Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"2019/04/02/Anaconda安装pyecharts可视化界面库/","link":"","permalink":"https://blog.chenyouguang.cn/2019/04/02/Anaconda%E5%AE%89%E8%A3%85pyecharts%E5%8F%AF%E8%A7%86%E5%8C%96%E7%95%8C%E9%9D%A2%E5%BA%93/","excerpt":"","text":"pyecharts 介绍 A Python Echarts Plotting Library.Chart: 30+ kinds of chartsMap: 300+ Chinese cities / 200+ countries and regionsPlatforms: Pure Python / Jupyter Notebook / Web Frameworkpyecharts 是一个用于生成 Echarts 图表的类库。Echarts 是百度开源的一个数据可视/Users/Apple/Documents/GitHub/blog/source/_posts/解决Mac下matplotlib中文不能显示问题.md化 JS 库。用 Echarts 生成的图可视化效果非常棒，pyecharts 是为了与 Python 进行对接，方便在 Python 中直接使用数据生成图。 pyecharts官网大家根据自己的使用平台采用不同的安装方式 Windows 本分支适合使用Windows系统的同学 打开anaconda prompt MacOS 本分支适合使用MacOS系统的同学 首先确定包管理器pip是anaconda的pip 1which pip out:/Users/Apple/anaconda3/bin/pip简单判断路径中含有anaconda即可 如果pip未指向anaconda中的pip(符合条件1的可跳过此步骤) 打开Anaconda-Navigator -&gt; Environments -&gt; base(也可以自己新建一个环境,默认是选择这个) -&gt; Open Terminal 此时，再执行步骤1，如果不行的话可以自行通过搜索引擎进行搜索解决 安装pyecharts 由于一些众所周知的原因，安装可能会有一点点慢。可以自行更换清华软件源解决 清华软件源-pypi镜像 1pip install pyecharts 安装地图数据 123456pip install echarts-countries-pypkgpip install echarts-china-provinces-pypkgpip install echarts-china-cities-pypkgpip install echarts-china-counties-pypkgpip install echarts-china-misc-pypkgpip install echarts-united-kingdom-pypkg","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/categories/Python/"}],"tags":[]},{"title":"解决Mac下matplotlib中文不能显示问题","slug":"解决Mac下matplotlib中文不能显示问题","date":"2019-03-26T03:11:35.000Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"2019/03/26/解决Mac下matplotlib中文不能显示问题/","link":"","permalink":"https://blog.chenyouguang.cn/2019/03/26/%E8%A7%A3%E5%86%B3Mac%E4%B8%8Bmatplotlib%E4%B8%AD%E6%96%87%E4%B8%8D%E8%83%BD%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","excerpt":"","text":"未解决前可以看到未解决前，中文字体显示为一个个矩形边框 下载字体首先我们先下载缺失的字体文件，任选一个下载即可fontpalace百度云 提取码: 6m92 如果您需要对图表商业化使用，下面有一些免费的开源字体列表 开源字体列表(English) 开源字体列表1(中文) 开源字体列表2(中文) 获取matplotlib的存放地址 进入python控制台执行以下命令获取文件路径 12import matplotlibmatplotlib.matplotlib_fname() 控制台输出中 /Users/Apple/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc 是matplotlib的存放路径 使用 Finder -&gt; 前往 -&gt; 前往文件夹将下载好的字体文件保存到fonts/ttf文件夹下 删除缓存文件打开控制台执行以下两个命令 12rm -rf ~/.matplotlib/*.cacherm -rf ~/.matplotlib/fontList.json 重建字体缓存 进入python控制台执行以下命令 12from matplotlib.font_manager import _rebuild_rebuild() 大功告成重新运行一下绘图程序 参考资料Mac os中matplotlib中文乱码问题Mac系统下matplotlib显示中文","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/categories/Python/"}],"tags":[{"name":"中文乱码","slug":"中文乱码","permalink":"https://blog.chenyouguang.cn/tags/%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"}]},{"title":"树莓派3B串口配置教程","slug":"树莓派3B串口配置教程","date":"2018-11-21T12:33:55.000Z","updated":"2020-06-28T16:29:25.849Z","comments":true,"path":"2018/11/21/树莓派3B串口配置教程/","link":"","permalink":"https://blog.chenyouguang.cn/2018/11/21/%E6%A0%91%E8%8E%93%E6%B4%BE3B%E4%B8%B2%E5%8F%A3%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/","excerpt":"","text":"树莓派3B串口配置教程前言 为什么需要配置串口？直接不能用吗？树莓派有两个串口，一个是硬件串口(PL011 UART)，一个是迷你串口(mini-uart),在默认情况下，蓝牙模块会使用PL011串口也就是\b硬件串口，而迷你串口(mini-uart)用于控制台输出。迷你UART的波特率与VC4 GPU上VPU的核心频率相关联。这意味着当VPU频率调节器改变核心频率时，UART的波特率也会发生变化。这使得UART在默认状态下受限使用。 配置 本教程将参考官方文档\b\b教大家配置树莓派系统: raspbian-stretch 官方文档(英文) 官方文档(英文) 树莓派中也有配置文档 到达overlays文件夹 1cd /boot/overlays 查看全部文档 1cat README 查看单个文档 1dtoverlay -h pi3-miniuart-bt 这里把文档贴出来Name: pi3-miniuart-btInfo: Switch Pi3 Bluetooth function to use the mini-UART (ttyS0) and restore UART0/ttyAMA0 over GPIOs 14 &amp; 15. Note that this may reduce the maximum usable baudrate.N.B. It is also necessary to edit /lib/systemd/system/hciuart.service and replace ttyAMA0 with ttyS0, unless you have a system with udev rules that create /dev/serial0 and /dev/serial1, in which case use/dev/serial1 instead because it will always be correct. Furthermore,you must also set core_freq=250 in config.txt or the miniuart will notwork.Usage: dtoverlay=pi3-miniuart-btParams: 教程开始1. 通过SSH连接树莓派2. 查看串口状态1ls -l /dev 结果将会看到所有接口 找到以下串口 12lrwxrwxrwx 1 root root 7 Nov 21 12:14 serial0 -&gt; ttyS0lrwxrwxrwx 1 root root 5 Nov 21 12:14 serial1 -&gt; ttyAMA0 3. 关闭板载蓝牙1sudo systemctl disable hciuart 4.修改 hciuart.service 文件路径 /lib/systemd/system/hciuart.service将文件中 ttyAMA0 替换为 ttyS0 如果hciuart.service中没有ttyAMA0可以跳过本步骤！ 1sudo vim /lib/systemd/system/hciuart.service 5. 修改 cmdline.txt 文件路径 /boot/cmdline.txt (修改前注意备份)删除掉 console=ttyAMA0,115200 kgdboc=ttyAMA0,115200 123cd /bootsudo cp cmdline.txt cmdline.txtbakvim cmdline.txt 修改为以下配置 (参考) 1dwc_otg.lpm_enable=0 console=tty1 root=PARTUUID=d71c341c-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles 6. 修改 config.txt 文件路径 /boot/config.txt (修改前注意备份)在尾部添加一行 dtoverlay=pi3-miniuart-bt\b如果存在 dtoverlay=pi3-disable-bt 请在前面加 # 注释掉\b据说如果同时存在两行串口会不能使用 123cd /bootsudo cp config.txt config.txtbakvim config.txt 7. 重启 重启 1sudo reboot 重启后查看串口接口 123ls -l /devlrwxrwxrwx 1 root root 7 Nov 21 12:14 serial0 -&gt; ttyAMA0lrwxrwxrwx 1 root root 5 Nov 21 12:14 serial1 -&gt; ttyS0 \b如果结果一样证明已经配置成功，诺出现不能开机，请\b自行\b恢复备份文件！ 参考 https://www.raspberrypi.org/documentation/configuration/uart.md https://www.jianshu.com/p/12d4f6882371 https://zhuanlan.zhihu.com/p/35312687 https://blog.csdn.net/asukasmallriver/article/details/76851375","categories":[{"name":"树莓派","slug":"树莓派","permalink":"https://blog.chenyouguang.cn/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"}],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://blog.chenyouguang.cn/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"串口","slug":"串口","permalink":"https://blog.chenyouguang.cn/tags/%E4%B8%B2%E5%8F%A3/"}]},{"title":"一种基于RFID的无人图书馆解决方案","slug":"一种基于RFID的无人图书馆解决方案","date":"2018-06-01T04:00:23.000Z","updated":"2020-06-27T11:23:19.000Z","comments":true,"path":"2018/06/01/一种基于RFID的无人图书馆解决方案/","link":"","permalink":"https://blog.chenyouguang.cn/2018/06/01/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8ERFID%E7%9A%84%E6%97%A0%E4%BA%BA%E5%9B%BE%E4%B9%A6%E9%A6%86%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"​ 生活、学习中的图书借阅和归还是比较费时费力的，有时候往往会因为图书馆人流拥挤，闭馆整理或者网电断停等等原因，无法顺利完成图书借阅，借书人和图书馆双方都会因种种不便承受各种损失。为此，基于物联网、无线传感器和云服务计算技术，我设计并开发了一种RFID智慧图书馆系统。","text":"​ 生活、学习中的图书借阅和归还是比较费时费力的，有时候往往会因为图书馆人流拥挤，闭馆整理或者网电断停等等原因，无法顺利完成图书借阅，借书人和图书馆双方都会因种种不便承受各种损失。为此，基于物联网、无线传感器和云服务计算技术，我设计并开发了一种RFID智慧图书馆系统。 1 研究背景​ 随着人工智能概念的日渐火爆及国家2025规划的推进，传统管理服务行业很难满足广大人群的有效需求。将现代管理及互联网与新一代信息技术应用于无人智能管理服务，最终使广大人群实现生活学习的不断便捷化、高效化。 ​ 人工智能，是智能化、自动化、信息化相结合的产物。在全球产业寻求变革的关键时刻，传统服务产业要紧跟时代潮流，结合行业特点，融合互联网特色，摸索出符合自身行业的发展模式才能在这瞬息巨变的时代优质存活并长足发展。 ​ 生活、学习中的图书借阅和归还是比较费时费力的，有时候往往会因为图书馆人流拥挤，闭馆整理或者网电断停等等原因，无法顺利完成图书借阅，借书人和图书馆双方都会因种种不便承受各种损失。为此，基于物联网、无线传感器和云服务计算技术，我们开发了RFID智慧图书馆系统。 2 设计理念​ 智慧图书馆图书管理系统采用互联网信息处理行业前沿技术，致力于为传统图书馆过渡到现代化智慧图书馆而设计，打造一个方便快捷又能提升用户借阅书籍体验的智慧图书管理系统。简单来说就是将书籍贴上RFID标签并连接上互联网进行数据库信息管理，减少人工管理图书馆的劳动力度，减少书籍的信息误差，尽可能的减少图书馆运营成本，让图书馆高效自行运作起来。 3 特色和创新(1)多元交互界面 ​ 相比以往传统书籍借阅，智慧图书馆系统界面风格极具特色，卡通元素，自然景，古典音乐等，流光溢彩，多轮转换。无需总是面对以往馆内沉闷脸孔和肃穆气氛， 大享欢快活泼借阅之旅。 (2)紧跟人工智能潮流 ​ 采用互联网信息处理技术，并进行数据深入分析挖掘。根据用户借阅图书风格和 种类，经过数据分析比对后，在交互界面中展现同类好书推荐和新书尝鲜。 (3)功能应用模块化设计 ​ 针对用户属性开发了管理端和用户端，分开设计。 ​ 管理端:管理功能一一俱全，图书借阅信息流量全天候监控，一目了然。用户信 息列表自动检索。终端设备状态实时报告等。 ​ 用户端:采用现代支付方式借阅书籍，新颖高效。借阅信息一览无余，归还时间 温馨􏰀醒，并开发书海探奇，书友家园等模块享有多元互动，好书推荐，实用备选， 新书尝鲜等功能。用户借阅之旅不亦乐乎。 (4)社会效益巨大 ​ 通过新型图书借阅模式，科技创新，畅享借阅乐趣，培养阅读爱好，引领阅读风尚，读书求学，经世致用，国民素质不断􏰀升。 4 系统架构4.1 基本架构 ​ 智慧图书馆系统基本架构由4个部分组成：后台管理部分、用户客户端部分、图书设备终端部分、云服务器部分，并由云服务器进行统一数据管理和统计。 (1) 后台管理端 ​ 后台管理端采用网页方式进行管理，使用网页管理拥有非常好跨平台性能。管理员使用任意一个拥有浏览器功能的操作系统进入后台管理系统即可实现全馆图书资料的录入、修改、删除等管理。 (2) 云端 ​ 以数据的存储和管理为核心的云计算系统，为应用提供接口，权限控制。使用者可以在任何时间，任何地点，通过任何可连接的装置连接到云端服务器方便的存取数据。 (3) 图书设备终端 ​ 无线识别感应器感应 RFID 芯片，把图书整体信息写入超低成本的 RFID 芯片并粘贴在图书中，高聚合融合成一体，可反复使用，供图书终端设备扫描。 (4) 用户移动客户端 ​ 用户使用移动客户端，可以识别图书终端设备生成的二维码订单并扫描完成图书借阅和归还，还可以根据用户借阅风格推荐相关书籍，分享读书心得互相交流。 4.2 系统模型 ​ 智慧图书馆采用分层分模块化设计，其中通过感知层用来标示和感知事物，是系统使用的入口，将采集到的书籍信息通过网络层，使用无线网络上传至云计算平台 中，在云计算平台搭建自己的服务器，将数据信息进行统一管理和维护。 4.3 信息传递流程 ​ 用户所持书籍内有RFID电子标签，图书终端设备中的读卡器会自动识别并读取RFID电子标签信息（最多可以同时识别50个电子标签），上传至云服务器中完成信息处理，并生成二维码订单，用户打开手机智慧图书馆移动APP扫描二维码完成订单确认，云服务器就会将用户信息和用户所借阅书籍信息统一管理起来，完成图书借阅。整个图书借阅和图书归还流程都无需人工干预，便于节省人工服务费用，使用机器管理，可以大大延长图书馆开馆时间，降低信息采集错误概率。以传感器、RFID 芯片、云服务器、后台智能管理端以及客户端设备之间的信息数据相互传递为基础，借此来完成实时、高效、快捷、一体化的图书借阅流程，让使用者倍感轻松。 (1)借阅图书流程 ​ 用户在图书馆借阅图书也无需繁琐的操作（如 图3.5借阅流程图 ），仅需要将需要借阅的书籍，带到智慧图书馆设备端读卡器识别区内，读卡器会自动识别书籍中的RFID芯片并采集数据信息上传到云服务器中，同时生成二维码订单返回到设备终端显示器上，用户使用智慧图书馆移动应用扫描二维码，会提示用户完成图书信息订单确认，此后云服务器会将用户和书籍关联起来，录入数据库中，给出提示信息。 (2)归还图书流程 ​ 用户将待归还的图书通过智慧图书馆设备读卡区，读卡器会自动识别 RFID 信息，并在云服务器中记录图书归还信息，提示用户归还图书成功。 4.4 云服务器4.4.1 云应用架构 ​ 云服务器采用Java和SpringBoot框架构建RESTful风格应用编程接口，使用MVC的设计理念，组件化微服务开发，并为不同的应用编写不同的接口控制组件。采用统一登录注册，并对用户身份进行权限验证。图书修改组件、图书查询组件都继承于图书管理组件，且图书修改信息操作需要对管理员的权限进行验证，由图书管理组件连接对象存储系统进行图片管理。订单管理组件用于管理图书订单，每次借阅操作都会创建一个订单，一个订单可以有多本图书。设备管理组件主要用于记录和操作无人图书馆设备硬件端，为图书设备提供访问云数据库的接口。数据库访问组件采用MyBatis作为持久层框架用于访问关系型数据库MySQL，便于存储和管理应用和用户数据。日志组件采用文本日志，对每项关键操作、系统错误进行记录，为开发人员分析数据和恢复数据提供强有力的保障，提升了系统的容错率。 4.4.2 数据库存储设计 ​ 数据库采用目前最受欢迎的开源关系型数据库MySQL。为每个表信息存储建立唯一主键，以及有关联的表之间采用数据库外键进行关联，为以后数据维护提供保障。 4.4.3 云服务器部署 ​ 使用CDN分布式网站站点加速，用户进行请求时，会对用户IP进行解析，采用分布式系统架构和高性能缓存软件，并由最近的CDN服务器进行命中数据，若数据命中即可直接从静态资源库中将资源推向用户，这样将大大减少当个云服务器的流量负担，加快了访问速度，只有未命中的请求会由CDN服务器向源服务器进行请求，同时在流量闲置时会自动检查与源站的数据缓存同步。 ​ 智慧图书馆云服务器中采用Docker虚拟化技术，将应用封装到当个容器中，创建私有网桥，仅有互相连接的部分才能访问到网络。反向代理Nginx中的端口是对外界进行监听，再由反向代理进行流量进行分流，静态资源将直接从服务器高速磁盘中读取，应用接口请求会转发到无人图书馆云应用中，再由无人图书馆云应用进行权限校验，对数据库的存储取出，网络之间是互相隐匿的，只有暴露连接部分连通，大大提高了安全性。 4.5 后台管理网页 ​ 图书馆管理员通过后台管理端对图书馆进行管理，使用Vue前端框架进行页面渲染，后台管理端拥有权限验证用于验证管理员身份，首页控制面板可以对于图书馆的情况一目了然，日销售量，日访问量，支付笔数等等一览无余，使用图书管理模块可以通过图书ISBN号码进行快速添加图书，并为图书添加电子标签。借阅记录的管理，可以通过二维码反馈信息（其中包含借阅人员姓名，借阅时间，借阅书籍资料等）进行借阅记录的保存、排序、整理、检索等，并且能导出表格格式的电子文档，从而实现借阅记录的管理。日志查询可以对用户使用情况进行分析。​ 后台管理网页采用前后端分离的开发方式，采用MVVC设计思想，将视图层（View）、显示模型层（ViewModel）、控制层（Control）、模型层（Model）分层，显示层采用Vue框架采用前后端分离等技术实现，使用Webpack技术进行打包，仅需要与服务器请求少量数据，大大减少服务器负载，再由前端框架进行本地的页面渲染，路由切换等等，页面切换基本无延迟，拥有良好的用户体验。 4.6 移动应用端4.6.1 信息关联 ​ 智慧图书设备读取到图书中的RFID芯片信息后，服务器将会把书籍的信息与一个新创建的订单关联，并且生成一个包含订单信息的二维码，此时用户打开智慧图书馆移动应用，使用扫一扫功能，扫描订单二维码，然后会向云服务器发送用户的唯一标示信息，此时云服务器就会将用户和订单关联起来完成借阅。​ 移动应用端作为智慧图书馆的用户入口，其主要的功能是标示用户，将用户和所借阅的图书关联起来。 4.6.2 生态圈 ​ 生态圈的建立对于应用来说是重中之重的，主要是通过用户参与各项活动，并且活动之间又互相关联。智慧图书馆生态社区主要是四大部分：图书推荐、图书借阅、读书心得分享、社区建设。用户使用智慧图书管理系统进行借阅后，可以在移动应用中分享自己的读书心得体会，并在社区中向大家分享，其他用户也同样参与讨论与分享，即可通过社区分享分析猜测用户可能喜爱的书籍，并向用户进行推荐图书，用户查阅到相关图书信息，就会有大概率进行再次借阅。 ​ 智慧图书馆的建设离不开用户的参与，移动应用作为智慧图书馆用户的入口，必定拥有较多的用户流量，通过建立智慧图书馆生态社区，更有利于增加用户粘性，使得更多的用户参与进来，从而达到良性循环。 4.7 图书设备端4.7.1 设计方向​ 智慧图书馆设备端是用于感知用户将要借阅的图书信息，同时将识别到的信息通过网络传递给云服务器，由云服务器进行订单的生成，并返回二维码信息，再由设备端进行显示，等待用户进行扫描二维码。 4.7.2 功能架构 (1)技术选型 ​ 智慧图书馆设备端的主控选为树莓派3B，树莓派可以运行Linux系统，并有丰富的接口设计，4个USB接口、HDMI显示接口、40针GPIO控制针，非常利于进行嵌入式开发。 ​ 智慧图书馆设备应用使用Java进行开发，Java有非常好的跨平台特点，以及不错的执行速度，并且有丰富的社区支持。 ​ 用户界面采用网页的形式，使用Vue前端框架进行构建，可以利用网页就能轻松构建较为丰富的用户界面交互。 (2)信息传递流程 ​ 由智慧图书馆设备应用中的标签信息中间件通过串口协议RS232与RFID标签读卡器进行数据通信。当有RFID电子标签在RFID标签读卡器的识别访问内时，将会源源不断的将标签信息传递到标签信息中间件，由中间件对这些信息进行过滤处理，将错误、重复的传输包进行丢弃。将多个的标签信息加入到消息队列中，并逐个上传到服务器中创建订单，服务器返回订单二维码信息，再由WebSocket通信向前端进行消息发送，前端再进行页面的刷新渲染。 5 成果展示5.1 设备端 5.2 后台管理 5.3 手机端","categories":[{"name":"研究项目","slug":"研究项目","permalink":"https://blog.chenyouguang.cn/categories/%E7%A0%94%E7%A9%B6%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"无人图书馆","slug":"无人图书馆","permalink":"https://blog.chenyouguang.cn/tags/%E6%97%A0%E4%BA%BA%E5%9B%BE%E4%B9%A6%E9%A6%86/"},{"name":"RFID","slug":"RFID","permalink":"https://blog.chenyouguang.cn/tags/RFID/"},{"name":"物联网","slug":"物联网","permalink":"https://blog.chenyouguang.cn/tags/%E7%89%A9%E8%81%94%E7%BD%91/"},{"name":"云计算","slug":"云计算","permalink":"https://blog.chenyouguang.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]}],"categories":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/categories/Python/"},{"name":"研究项目","slug":"研究项目","permalink":"https://blog.chenyouguang.cn/categories/%E7%A0%94%E7%A9%B6%E9%A1%B9%E7%9B%AE/"},{"name":"树莓派","slug":"树莓派","permalink":"https://blog.chenyouguang.cn/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.chenyouguang.cn/tags/Python/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://blog.chenyouguang.cn/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"热力图","slug":"热力图","permalink":"https://blog.chenyouguang.cn/tags/%E7%83%AD%E5%8A%9B%E5%9B%BE/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://blog.chenyouguang.cn/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"中文乱码","slug":"中文乱码","permalink":"https://blog.chenyouguang.cn/tags/%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"},{"name":"树莓派","slug":"树莓派","permalink":"https://blog.chenyouguang.cn/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"串口","slug":"串口","permalink":"https://blog.chenyouguang.cn/tags/%E4%B8%B2%E5%8F%A3/"},{"name":"无人图书馆","slug":"无人图书馆","permalink":"https://blog.chenyouguang.cn/tags/%E6%97%A0%E4%BA%BA%E5%9B%BE%E4%B9%A6%E9%A6%86/"},{"name":"RFID","slug":"RFID","permalink":"https://blog.chenyouguang.cn/tags/RFID/"},{"name":"物联网","slug":"物联网","permalink":"https://blog.chenyouguang.cn/tags/%E7%89%A9%E8%81%94%E7%BD%91/"},{"name":"云计算","slug":"云计算","permalink":"https://blog.chenyouguang.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]}